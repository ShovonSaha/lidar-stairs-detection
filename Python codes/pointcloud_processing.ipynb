{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'open3d' has no attribute 'visualization' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Step 1: Imports and Utility Functions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrosbag\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mo3d\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msensor_msgs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoint_cloud2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc2\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/__init__.py:96\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJupyter environment detected. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnabling Open3D WebVisualizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Set default window system.\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mopen3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241m.\u001b[39mwebrtc_server\u001b[38;5;241m.\u001b[39menable_webrtc()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# HTTP handshake server is needed when Open3D is serving the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# visualizer webpage. Disable since Jupyter is serving.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m open3d\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mwebrtc_server\u001b[38;5;241m.\u001b[39mdisable_http_handshake()\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'open3d' has no attribute 'visualization' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Step 1: Imports and Utility Functions\n",
    "# -----------------------------------\n",
    "import rosbag\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import sensor_msgs.point_cloud2 as pc2\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "def ros_point_cloud2_to_o3d(point_cloud_msg):\n",
    "    \"\"\"\n",
    "    Convert a sensor_msgs/PointCloud2 message to an Open3D PointCloud.\n",
    "    \"\"\"\n",
    "    points = np.array(list(pc2.read_points(point_cloud_msg, skip_nans=True, field_names=(\"x\", \"y\", \"z\"))))\n",
    "    o3d_pcd = o3d.geometry.PointCloud()\n",
    "    o3d_pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    return o3d_pcd\n",
    "\n",
    "def save_processed_point_cloud(o3d_pcd, output_directory, bag_file_name, msg_index):\n",
    "    \"\"\"\n",
    "    Save the processed Open3D point cloud to a file with a unique index.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    output_file_path = os.path.join(output_directory, f\"{os.path.splitext(bag_file_name)[0]}_{msg_index}.pcd\")\n",
    "    o3d.io.write_point_cloud(output_file_path, o3d_pcd)\n",
    "    print(f\"Saved {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Step 2: Define Processing Function\n",
    "# -----------------------------------\n",
    "def process_rosbag(bag_file, topic_name, base_output_directory, error_log_file):\n",
    "    \"\"\"\n",
    "    Process a ROS bag file to convert all messages on a topic to PCD files and aggregate them.\n",
    "    If any error arises, log the error with the file name into a CSV file.\n",
    "    \"\"\"\n",
    "    bag_name = os.path.splitext(os.path.basename(bag_file))[0]\n",
    "    output_directory = os.path.join(base_output_directory, bag_name)\n",
    "    \n",
    "    # Skip processing if the file has already been processed\n",
    "    grand_pcd_path = os.path.join(output_directory, f\"{bag_name}_grand.pcd\")\n",
    "    if os.path.exists(grand_pcd_path):\n",
    "        print(f\"{bag_file} already processed. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "\n",
    "        bag = rosbag.Bag(bag_file, \"r\")\n",
    "        all_pcds = []\n",
    "        for index, (topic, msg, t) in enumerate(bag.read_messages(topics=[topic_name])):\n",
    "            o3d_pcd = ros_point_cloud2_to_o3d(msg)\n",
    "            save_processed_point_cloud(o3d_pcd, output_directory, bag_name, index)\n",
    "            all_pcds.append(o3d_pcd)\n",
    "        bag.close()\n",
    "\n",
    "        # Aggregate all PCDs into one grand PCD file\n",
    "        grand_pcd = o3d.geometry.PointCloud()\n",
    "        for pcd in all_pcds:\n",
    "            grand_pcd += pcd\n",
    "        o3d.io.write_point_cloud(grand_pcd_path, grand_pcd)\n",
    "        print(f\"Aggregated PCD saved to {grand_pcd_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {bag_file}: {e}\")\n",
    "        with open(error_log_file, mode='a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([bag_file, str(e)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Step 3: Set Directory Paths and Topic Name\n",
    "# -----------------------------------\n",
    "source_directory = os.path.expanduser(\"/home/nrelab-titan/Desktop/shovon/data/rosbags_cyglidar/rosbags_for_training\")\n",
    "base_output_directory = os.path.join(source_directory, \"processed_pointclouds_combined\")\n",
    "point_cloud_topic = \"/scan_3D\"\n",
    "error_log_file = os.path.join(base_output_directory, \"error_log.csv\")\n",
    "\n",
    "# Ensure the error log file exists\n",
    "if not os.path.exists(base_output_directory):\n",
    "    os.makedirs(base_output_directory)\n",
    "\n",
    "if not os.path.exists(error_log_file):\n",
    "    with open(error_log_file, mode='w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Bag File\", \"Error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Step 4: Process Each Bag File in Subdirectories\n",
    "# ---------------------------------------\n",
    "folders = [\"stairs\", \"non_stairs\"]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(source_directory, folder)\n",
    "    output_folder_path = os.path.join(base_output_directory, folder)\n",
    "    \n",
    "    bag_files = glob.glob(os.path.join(folder_path, \"*.bag\"))\n",
    "    print(f\"Found {len(bag_files)} rosbag(s) in {folder} to process.\")\n",
    "    \n",
    "    for bag_file in bag_files:\n",
    "        print(f\"Processing {bag_file} in {folder}...\")\n",
    "        process_rosbag(bag_file, point_cloud_topic, output_folder_path, error_log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Step 5: Define Downsampling Functions\n",
    "# ---------------------------------------\n",
    "def downsample_point_cloud(pcd, voxel_size):\n",
    "    \"\"\"\n",
    "    Downsample the given point cloud using a voxel grid filter with the specified voxel size.\n",
    "    \"\"\"\n",
    "    downsampled_pcd = pcd.voxel_down_sample(voxel_size)\n",
    "    return downsampled_pcd\n",
    "\n",
    "def downsample_pcd_files(input_directory, output_directory, voxel_size=0.05, error_log_file=\"downsample_error_log.csv\"):\n",
    "    \"\"\"\n",
    "    Downsample all PCD files in the input directory and save them to the output directory.\n",
    "    Log any errors to a CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    if not os.path.exists(error_log_file):\n",
    "        with open(error_log_file, mode='w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"PCD File\", \"Error\"])\n",
    "    \n",
    "    pcd_files = glob.glob(os.path.join(input_directory, \"*.pcd\"))\n",
    "    for pcd_file in pcd_files:\n",
    "        try:\n",
    "            print(f\"Reading {pcd_file}\")\n",
    "            pcd = o3d.io.read_point_cloud(pcd_file)\n",
    "            downsampled_pcd = downsample_point_cloud(pcd, voxel_size)\n",
    "            output_file_path = os.path.join(output_directory, os.path.basename(pcd_file))\n",
    "            o3d.io.write_point_cloud(output_file_path, downsampled_pcd)\n",
    "            print(f\"Downsampled and saved {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pcd_file}: {e}\")\n",
    "            with open(error_log_file, mode='a') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([pcd_file, str(e)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Step 6: Set Downsampled Output Directory\n",
    "# ---------------------------------------\n",
    "downsampled_output_directory = os.path.expanduser(\"/home/nrelab-titan/Desktop/shovon/data/rosbags_cyglidar/rosbags_for_training/downsampled\")\n",
    "\n",
    "# Ensure that the downsampled output directory exists\n",
    "if not os.path.exists(downsampled_output_directory):\n",
    "    os.makedirs(downsampled_output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 7: Downsample PCD Files in Subdirectories\n",
    "# ----------------------------------------------\n",
    "folders = [\"stairs\", \"non_stairs\"]\n",
    "error_log_file = os.path.join(downsampled_output_directory, \"downsample_error_log.csv\")\n",
    "\n",
    "for folder in folders:\n",
    "    subfolders = os.listdir(os.path.join(base_output_directory, folder))\n",
    "    for subfolder in subfolders:\n",
    "        processed_folder_path = os.path.join(base_output_directory, folder, subfolder)\n",
    "        downsampled_folder_path = os.path.join(downsampled_output_directory, folder, subfolder)\n",
    "        \n",
    "        print(f\"Downsampling PCD files in {folder}/{subfolder}...\")\n",
    "        downsample_pcd_files(processed_folder_path, downsampled_folder_path, voxel_size=0.05, error_log_file=error_log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 8: Define PCA Function\n",
    "# ----------------------------------------------\n",
    "def perform_pca(pcd):\n",
    "    \"\"\"\n",
    "    Perform PCA on the given point cloud.\n",
    "    Returns eigenvalues and eigenvectors.\n",
    "    \"\"\"\n",
    "    points = np.asarray(pcd.points)\n",
    "    if points.shape[0] < 50:  # Check if there are enough points (threshold is 50)\n",
    "        raise ValueError(\"Point cloud does not have enough points for PCA\")\n",
    "    \n",
    "    cov_matrix = np.cov(points, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    # Sort the eigenvalues and eigenvectors\n",
    "    sort_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sort_indices]\n",
    "    eigenvectors = eigenvectors[:, sort_indices]\n",
    "    return eigenvalues, eigenvectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 9: Define PCA Processing Function\n",
    "# ----------------------------------------------\n",
    "def process_pca_for_downsampled_pcds(input_directory, output_csv):\n",
    "    \"\"\"\n",
    "    Perform PCA on all downsampled PCD files in the input directory and store results in a CSV file.\n",
    "    Log any errors and files with insufficient points to separate CSV files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(output_csv)):\n",
    "        os.makedirs(os.path.dirname(output_csv))\n",
    "    \n",
    "    error_log_file = os.path.join(os.path.dirname(output_csv), \"pca_error_log.csv\")\n",
    "    insufficient_points_log_file = os.path.join(os.path.dirname(output_csv), \"pca_insufficient_points_log.csv\")\n",
    "    \n",
    "    with open(output_csv, mode='w') as file, open(error_log_file, mode='w') as err_file, open(insufficient_points_log_file, mode='w') as ins_file:\n",
    "        writer = csv.writer(file)\n",
    "        err_writer = csv.writer(err_file)\n",
    "        ins_writer = csv.writer(ins_file)\n",
    "        \n",
    "        header = [\"File Name\", \"Label\"] + [f\"Eigenvalue_{i+1}\" for i in range(3)] + [f\"Eigenvector_{i+1}_X\" for i in range(3)] + [f\"Eigenvector_{i+1}_Y\" for i in range(3)] + [f\"Eigenvector_{i+1}_Z\" for i in range(3)]\n",
    "        writer.writerow(header)\n",
    "        err_writer.writerow([\"PCD File\", \"Error\"])\n",
    "        ins_writer.writerow([\"PCD File\", \"Number of Points\"])\n",
    "        \n",
    "        folders = {\"stairs\": 1, \"non_stairs\": 0}\n",
    "        for folder, label in folders.items():\n",
    "            subfolders = os.listdir(os.path.join(input_directory, folder))\n",
    "            for subfolder in subfolders:\n",
    "                pcd_files = glob.glob(os.path.join(input_directory, folder, subfolder, \"*.pcd\"))\n",
    "                for pcd_file in pcd_files:\n",
    "                    try:\n",
    "                        print(f\"Performing PCA on {pcd_file}\")\n",
    "                        pcd = o3d.io.read_point_cloud(pcd_file)\n",
    "                        points = np.asarray(pcd.points)\n",
    "                        if points.shape[0] < 50:\n",
    "                            raise ValueError(f\"Insufficient points: {points.shape[0]}\")\n",
    "                        eigenvalues, eigenvectors = perform_pca(pcd)\n",
    "                        row = [os.path.basename(pcd_file), label] + eigenvalues.tolist() + eigenvectors[:, 0].tolist() + eigenvectors[:, 1].tolist() + eigenvectors[:, 2].tolist()\n",
    "                        writer.writerow(row)\n",
    "                        print(f\"PCA results saved for {pcd_file}\")\n",
    "                    except ValueError as ve:\n",
    "                        if \"Insufficient points\" in str(ve):\n",
    "                            print(f\"Insufficient points for {pcd_file}: {points.shape[0]}\")\n",
    "                            ins_writer.writerow([pcd_file, points.shape[0]])\n",
    "                        else:\n",
    "                            print(f\"Error performing PCA on {pcd_file}: {ve}\")\n",
    "                            err_writer.writerow([pcd_file, str(ve)])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error performing PCA on {pcd_file}: {e}\")\n",
    "                        err_writer.writerow([pcd_file, str(e)])\n",
    "    \n",
    "    return error_log_file, insufficient_points_log_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 10: Set Output CSV Path and Perform PCA\n",
    "# ----------------------------------------------\n",
    "output_csv = os.path.join(source_directory, \"pca_results.csv\")\n",
    "error_log_file, insufficient_points_log_file = process_pca_for_downsampled_pcds(downsampled_output_directory, output_csv)\n",
    "\n",
    "# Print the number of rows in the error and insufficient points CSV files\n",
    "error_log = pd.read_csv(error_log_file)\n",
    "insufficient_points_log = pd.read_csv(insufficient_points_log_file)\n",
    "print(f\"Number of rows in error log: {len(error_log)}\")\n",
    "print(f\"Number of rows in insufficient points log: {len(insufficient_points_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 11: Import Necessary Libraries for Machine Learning\n",
    "# ----------------------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 12: Load and Preprocess the PCA Results\n",
    "# ----------------------------------------------\n",
    "# Load the PCA results CSV\n",
    "pca_results_csv = os.path.join(source_directory, \"pca_results.csv\")\n",
    "pca_data = pd.read_csv(pca_results_csv)\n",
    "\n",
    "# Extract features and labels\n",
    "X = pca_data.drop(columns=[\"File Name\", \"Label\"])\n",
    "y = pca_data[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 13: Perform 5-Fold Cross-Validation\n",
    "# ----------------------------------------------\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Set up the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and collect accuracy scores\n",
    "cv_scores = cross_val_score(svm_classifier, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores and mean accuracy\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 14: Train and Evaluate the Classifier with Detailed Metrics\n",
    "# ----------------------------------------------\n",
    "# Initialize lists to hold the results\n",
    "accuracy_list = []\n",
    "conf_matrix_list = []\n",
    "class_report_list = []\n",
    "\n",
    "# Perform manual k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the SVM classifier\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrix_list.append(conf_matrix)\n",
    "    \n",
    "    # Calculate classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    class_report_list.append(class_report)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean cross-validation accuracy: {np.mean(accuracy_list)}\")\n",
    "print(\"Confusion Matrices for each fold:\")\n",
    "for i, cm in enumerate(conf_matrix_list):\n",
    "    print(f\"Fold {i+1}:\\n{cm}\")\n",
    "print(\"Classification Reports for each fold:\")\n",
    "for i, cr in enumerate(class_report_list):\n",
    "    print(f\"Fold {i+1}:\\n{pd.DataFrame(cr).transpose()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Step 15: Perform PCA with 3 Components and Evaluate Accuracy\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# Reduce the data to 3D using PCA\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "# Train the SVM model on the 3D PCA-reduced data\n",
    "svm_classifier_3d = SVC(kernel='linear')\n",
    "svm_classifier_3d.fit(X_3d, y)\n",
    "\n",
    "# Evaluate the classifier with 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_3d = cross_val_score(svm_classifier_3d, X_3d, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores and mean accuracy\n",
    "print(f\"Cross-validation scores (3D PCA): {cv_scores_3d}\")\n",
    "print(f\"Mean cross-validation accuracy (3D PCA): {cv_scores_3d.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 15: Visualizing the SVM Decision Boundary\n",
    "# ----------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Reduce the data to 2D using PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X)\n",
    "\n",
    "# Train the SVM model with initialized parameters\n",
    "svm_classifier_initialized = SVC(kernel='linear')  # Using 'linear' kernel as in Step 13\n",
    "svm_classifier_initialized.fit(X_2d, y)\n",
    "\n",
    "# Plot the decision boundary\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = svm_classifier_initialized.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolors='k', marker='o')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('SVM Decision Boundary with PCA Reduced Data (Initialized Parameters)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Step 16: Hyperparameter Tuning with GridSearchCV (CPU-based)\n",
    "# ----------------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Define a parameter grid for SVM\n",
    "param_grid_cpu = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Set up the SVM classifier\n",
    "svm_classifier_cpu = SVC()\n",
    "\n",
    "# Set up RandomizedSearchCV with n_jobs=-1 to use all available cores and verbose=2 for detailed output\n",
    "random_search_cpu = RandomizedSearchCV(svm_classifier_cpu, param_distributions=param_grid_cpu, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42, verbose=2)\n",
    "\n",
    "try:\n",
    "    # Perform the random search\n",
    "    random_search_cpu.fit(X, y)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Process interrupted. Printing current results...\")\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params_cpu = random_search_cpu.best_params_ if hasattr(random_search_cpu, 'best_params_') else None\n",
    "best_score_cpu = random_search_cpu.best_score_ if hasattr(random_search_cpu, 'best_score_') else None\n",
    "\n",
    "print(f\"Best parameters (CPU): {best_params_cpu}\")\n",
    "print(f\"Best cross-validation accuracy (CPU): {best_score_cpu}\")\n",
    "\n",
    "# Save the search results\n",
    "if hasattr(random_search_cpu, 'cv_results_'):\n",
    "    cv_results_cpu_df = pd.DataFrame(random_search_cpu.cv_results_)\n",
    "    cv_results_cpu_df.to_csv('hyperparameter_tuning_results_cpu.csv', index=False)\n",
    "\n",
    "    # Define the minimum expected accuracy\n",
    "    min_expected_accuracy = 0.80\n",
    "\n",
    "    # Filter the results\n",
    "    filtered_results = cv_results_cpu_df[cv_results_cpu_df['mean_test_score'] >= min_expected_accuracy]\n",
    "\n",
    "    print(f\"Filtered results with accuracy >= {min_expected_accuracy * 100}%:\")\n",
    "    print(filtered_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------\n",
    "# # Step 17: Evaluate the Best Model with Detailed Metrics\n",
    "# # ----------------------------------------------\n",
    "# # Train the best model\n",
    "# best_svm_classifier = grid_search.best_estimator_\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize lists to hold the results\n",
    "# accuracy_list = []\n",
    "# conf_matrix_list = []\n",
    "# class_report_list = []\n",
    "\n",
    "# # Perform manual k-fold cross-validation\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     # Train the SVM classifier\n",
    "#     best_svm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on the test data\n",
    "#     y_pred = best_svm_classifier.predict(X_test)\n",
    "    \n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     accuracy_list.append(accuracy)\n",
    "    \n",
    "#     # Calculate confusion matrix\n",
    "#     conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#     conf_matrix_list.append(conf_matrix)\n",
    "    \n",
    "#     # Calculate classification report\n",
    "#     class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "#     class_report_list.append(class_report)\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Mean cross-validation accuracy: {np.mean(accuracy_list)}\")\n",
    "# print(\"Confusion Matrices for each fold:\")\n",
    "# for i, cm in enumerate(conf_matrix_list):\n",
    "#     print(f\"Fold {i+1}:\\n{cm}\")\n",
    "# print(\"Classification Reports for each fold:\")\n",
    "# for i, cr in enumerate(class_report_list):\n",
    "#     print(f\"Fold {i+1}:\\n{pd.DataFrame(cr).transpose()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
